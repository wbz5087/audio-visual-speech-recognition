{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam,SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Video import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "video = Video(path='save/')\n",
    "trainlist=video.generate_file_list(p='train')\n",
    "vallist = video.generate_file_list(p='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "testlist = video.generate_file_list(p='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "model=load_model('video_weights/weights-improvement-08-0.68.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n",
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "pred={}\n",
    "labels=os.listdir('save/')\n",
    "for i,label in enumerate(labels):\n",
    "    data,classes = video.load_data(label,'test')\n",
    "    result=model.predict(data)\n",
    "    result=np.argmax(result,axis=1)\n",
    "    classes=np.argmax(classes.toarray(),axis=1)\n",
    "    #print(classes==result)\n",
    "    acc=float(sum(result==classes))/float(len(classes))\n",
    "    #print(type(acc),type(label))\n",
    "    pred[label]=acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'save/INCREASE/test/INCREASE_00046'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y,classes=video.get_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fileObject = open('filelist.txt', 'w')\n",
    "for ip in trainlist:\n",
    "    fileObject.write(ip)\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()\n",
    "\n",
    "fileObject = open('vallist.txt', 'w')\n",
    "for ip in trainlist:\n",
    "    fileObject.write(ip)\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_list = []\n",
    "fd = open( \"filelist.txt\", \"r\" )\n",
    "\n",
    "for line in fd.readlines():\n",
    "    train_list.append(line.rstrip('\\n'))\n",
    "    \n",
    "val_list = []\n",
    "fd = open( \"vallist.txt\", \"r\" )\n",
    "\n",
    "for line in fd.readlines():\n",
    "    val_list.append(line.rstrip('\\n'))\n",
    "    #print(line)\n",
    "#print(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D,Conv3D\n",
    "from keras.layers.convolutional import MaxPooling2D,MaxPooling3D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import GlobalMaxPooling1D,GlobalMaxPooling2D\n",
    "from keras.layers import LSTM,Input,GlobalAveragePooling1D,GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "#from keras.layers.extra import TimeDistributedConvolution2D, TimeDistributedMaxPooling2D, TimeDistributedFlatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 29, 120, 120, 3)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 29, 60, 60, 64)    47104     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 29, 60, 60, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 29, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 29, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 29, 28, 28, 128)   73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 29, 28, 28, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 29, 14, 14, 128)   0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 29, 14, 14, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 29, 12, 12, 256)   295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 29, 12, 12, 256)   1024      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 29, 6, 6, 256)     0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 29, 6, 6, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 29, 4, 4, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 29, 4, 4, 512)     2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 29, 2, 2, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 29, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "avg (GlobalAveragePooling1D) (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               256500    \n",
      "=================================================================\n",
      "Total params: 3,696,244\n",
      "Trainable params: 3,693,300\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#inputShape = (29,120,120,3)\n",
    "input1 = Input(shape = (29,120,120,3))\n",
    "\n",
    "conv3d = Conv3D(64,(5,7,7),padding = 'same',\n",
    "                strides = (1,2,2), activation='relu')(input1)\n",
    "batch_norm1 = BatchNormalization()(conv3d)\n",
    "maxpool3d = MaxPooling3D(pool_size=(1,2,2),padding='same')(batch_norm1)\n",
    "drop1 = Dropout(0.1)(maxpool3d)\n",
    "\n",
    "time_conv2d = TimeDistributed(Conv2D(128,(3,3),activation = 'relu'))(drop1)\n",
    "batch_norm2 = BatchNormalization()(time_conv2d)\n",
    "time_maxpool2d = TimeDistributed(MaxPooling2D(2,2))(batch_norm2)\n",
    "drop2 = Dropout(0.1)(time_maxpool2d)\n",
    "\n",
    "time_conv2d = TimeDistributed(Conv2D(256,(3,3),activation = 'relu'))(drop2)\n",
    "batch_norm2 = BatchNormalization()(time_conv2d)\n",
    "time_maxpool2d = TimeDistributed(MaxPooling2D(2,2))(batch_norm2)\n",
    "drop2 = Dropout(0.1)(time_maxpool2d)\n",
    "\n",
    "time_conv2d = TimeDistributed(Conv2D(512,(3,3),activation = 'relu'))(drop2)\n",
    "batch_norm3 = BatchNormalization()(time_conv2d)\n",
    "time_maxpool2d = TimeDistributed(MaxPooling2D(2,2))(batch_norm3)\n",
    "drop3 = Dropout(0.1)(time_maxpool2d)\n",
    "\n",
    "time_flatten = TimeDistributed(GlobalAveragePooling2D())(time_maxpool2d)\n",
    "\n",
    "bilstm1 = Bidirectional(LSTM(256,return_sequences=True,dropout=0.1,\n",
    "                            name='bilstm'))(time_flatten)\n",
    "#bilstm2 = Bidirectional(LSTM(128,return_sequences=True))(bilstm1)\n",
    "\n",
    "avg = GlobalAveragePooling1D(name='avg')(bilstm1)\n",
    "dense1 = Dense(512,activation='relu')(avg)\n",
    "batch_norm4 = BatchNormalization()(dense1)\n",
    "dense2 = Dense(class_num,activation = 'softmax')(batch_norm4)\n",
    "\n",
    "model = Model(inputs=[input1], \n",
    "              outputs=[dense2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 29, 120, 120, 3)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 29, 60, 60, 64)    47104     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 29, 60, 60, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 29, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 29, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 29, 28, 28, 128)   73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 29, 28, 28, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 29, 14, 14, 128)   0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 29, 14, 14, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 29, 12, 12, 256)   295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 29, 12, 12, 256)   1024      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 29, 6, 6, 256)     0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 29, 6, 6, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 29, 4, 4, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 29, 4, 4, 512)     2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 29, 2, 2, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 29, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 29, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "avg (GlobalAveragePooling1D) (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               256500    \n",
      "=================================================================\n",
      "Total params: 3,696,244\n",
      "Trainable params: 3,693,300\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input1], \n",
    "              outputs=[dense2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# EPOCHS = 3\n",
    "# INIT_LR = 0.1\n",
    "# BS = 16\n",
    "# IMAGE_DIMS = (120,120,29)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range = 10, zoom_range=0.2,\n",
    "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
    "# aug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_5_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=5349332, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_5_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op u'time_distributed_5_1/random_uniform/RandomUniform', defined at:\n  File \"/home/oscar/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/oscar/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-890a52a46700>\", line 20, in <module>\n    time_conv2d = TimeDistributed(Conv2D(512,(3,3),activation = 'relu'))(drop2)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 592, in __call__\n    self.build(input_shapes[0])\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/layers/wrappers.py\", line 168, in build\n    self.layer.build(child_input_shape)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 138, in build\n    constraint=self.kernel_constraint)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 413, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/initializers.py\", line 217, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3838, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 242, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 672, in random_uniform\n    name=name)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_5_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=5349332, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_5_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4cca130b2202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'video_weights/weights-improvement-08-0.68.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                 load_weights_from_hdf5_group(\n\u001b[0;32m-> 2667\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   3391\u001b[0m                              ' elements.')\n\u001b[1;32m   3392\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3393\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2375\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2377\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_5_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=5349332, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_5_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op u'time_distributed_5_1/random_uniform/RandomUniform', defined at:\n  File \"/home/oscar/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/oscar/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-890a52a46700>\", line 20, in <module>\n    time_conv2d = TimeDistributed(Conv2D(512,(3,3),activation = 'relu'))(drop2)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 592, in __call__\n    self.build(input_shapes[0])\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/layers/wrappers.py\", line 168, in build\n    self.layer.build(child_input_shape)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 138, in build\n    constraint=self.kernel_constraint)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 413, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/initializers.py\", line 217, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3838, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 242, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 672, in random_uniform\n    name=name)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/oscar/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,256,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_5_1/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=5349332, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_5_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('video_weights/weights-improvement-08-0.68.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "# model = SmallerVGGNet.build(width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
    "# \tdepth=IMAGE_DIMS[2], classes=6)\n",
    "#opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "sgd = SGD(lr=0.004992, decay=1e-6, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fit(trainX, train_Y, epochs=1, batch_size=32, \n",
    "          verbose=1, validation_data=(valX, val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[INFO] training network...\")\n",
    "# H = model.fit_generator(\n",
    "# \taug.flow(trainX, train_Y, batch_size=32),\n",
    "# \tvalidation_data=(testX, test_Y),\n",
    "# \tsteps_per_epoch=len(trainX) // 32,\n",
    "# \tepochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "valX = np.reshape(valX,(-1,120,120,29))\n",
    "testX = np.reshape(testX,(-1,120,120,29))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fit(valX, val_Y, epochs=10, batch_size=16, \n",
    "          verbose=1, validation_data=(testX, test_Y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/3\n",
      " 6356/10182 [=================>............] - ETA: 4:08:04 - loss: 0.8420 - acc: 0.7590save/LIVING/train/LIVING_00834\n",
      " 9317/10182 [==========================>...] - ETA: 57:37 - loss: 0.8251 - acc: 0.7635save/LIVING/train/LIVING_00834\n",
      "10182/10182 [==============================] - 43342s 4s/step - loss: 0.8186 - acc: 0.7653 - val_loss: 1.2533 - val_acc: 0.6712\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67123, saving model to video_weights/weights-improvement-01-0.67.hdf5\n",
      "Epoch 2/3\n",
      " 3319/10182 [========>.....................] - ETA: 8:09:22 - loss: 0.7903 - acc: 0.7727save/LIVING/train/LIVING_00834\n",
      " 9034/10182 [=========================>....] - ETA: 1:17:32 - loss: 0.7714 - acc: 0.7770save/LIVING/train/LIVING_00834\n",
      "10182/10182 [==============================] - 43319s 4s/step - loss: 0.7620 - acc: 0.7796 - val_loss: 1.2198 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67123 to 0.68534, saving model to video_weights/weights-improvement-02-0.69.hdf5\n",
      "Epoch 3/3\n",
      " 5594/10182 [===============>..............] - ETA: 5:00:46 - loss: 0.7338 - acc: 0.7869save/LIVING/train/LIVING_00834\n",
      "10182/10182 [==============================] - 42421s 4s/step - loss: 0.7060 - acc: 0.7943 - val_loss: 1.2671 - val_acc: 0.6748\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.68534\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training network...\")\n",
    "sample_number = len(trainlist)\n",
    "val_number = len(vallist)\n",
    "#print(sample_number)\n",
    "filepath=\"video_weights/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,\n",
    "mode='max')\n",
    "stopper = EarlyStopping(monitor='val_acc',patience=3)\n",
    "callbacks_list = [checkpoint,stopper]\n",
    "\n",
    "history = model.fit_generator(video.generate_array(trainlist,batch_size=48),\n",
    "                        validation_data=\n",
    "                    video.generate_array(vallist,batch_size=48),\n",
    "                        steps_per_epoch=sample_number // 48,\n",
    "                    validation_steps = val_number//48,\n",
    "                        epochs=3, verbose=1,\n",
    "                   callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'loss', 'val_acc', 'val_loss']\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff82040bf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.savefig('visual9-12acc.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff8206c7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'],loc='upper left')\n",
    "plt.savefig('visual9-12loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('video_weights/weights-improvement-08-0.68.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2596036778046535, 0.6676282056249105]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_weights('weights-improvement-04-0.98.hdf5')\n",
    "model.evaluate_generator(video.generate_array(testlist,batch_size=48),\n",
    "                         steps=len(testlist)//48)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_data(path,p):\n",
    "    print(\"[INFO] loading images...\")\n",
    "    data = []\n",
    "    labels = []\n",
    "    train_y = []\n",
    "    # grab the image paths and randomly shuffle them\n",
    "    \n",
    "    labels = os.listdir(path)\n",
    "    #labels.remove('.DS_Store')\n",
    "    imagePaths = []\n",
    "    print(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "        path_file=os.path.join(path, label)\n",
    "        #path_file = os.path.join(path,'label')\n",
    "        path_files = os.path.join(path_file,p)\n",
    "        imagePath = os.listdir(path_files)\n",
    "        #print(path_files)\n",
    "        #imagePath.remove('.DS_Store')\n",
    "        for img in imagePath:\n",
    "            if img == '.DS_Store':\n",
    "                print('wrong')\n",
    "            else:\n",
    "                image = os.path.join(path_files,img)\n",
    "                imagePaths.append(image)\n",
    "            #print(path_files)\n",
    "    #print(imagePaths)\n",
    "    random.seed(42)\n",
    "    random.shuffle(imagePaths)\n",
    "    #print(imagePaths)\n",
    "    # loop over the input images\n",
    "    for imagePath in imagePaths:\n",
    "        image_path = sorted(list(paths.list_images(imagePath)))\n",
    "        images=[]\n",
    "        for img in image_path:\n",
    "            \n",
    "            # load the image, pre-process it, and store it in the data list\n",
    "            image = cv2.imread(img)\n",
    "            #image = cv2.resize(image, (norm_size, norm_size))\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            #image = cv2.resize(image, (norm_size, norm_size))\n",
    "            image = img_to_array(image)\n",
    "            #print(image)\n",
    "            #image = np.array(image, dtype=\"float\") / 255.0\n",
    "            #print(image.shape)\n",
    "            image = np.reshape(image,(120,120,3))\n",
    "            images.append(image)\n",
    "        \n",
    "        images=np.array(images,dtype=\"float\")\n",
    "        if(len(images)==0):\n",
    "            print(imagePath)\n",
    "            break\n",
    "        #images=np.reshape(images,(120,120,29))\n",
    "        #print(images.shape)\n",
    "        #print(images.shape)\n",
    "        data.append(images)\n",
    "\n",
    "        # extract the class label from the image path and update the\n",
    "        # labels list\n",
    "        #label = int(imagePath.split(os.path.sep)[-2])\n",
    "        basename = os.path.basename(imagePath)\n",
    "        label = basename.split('_')\n",
    "        train_y.append(label[0])\n",
    "        #print(len(labels))\n",
    "    #data = np.array(data, dtype=\"float\") / 255.0\n",
    "    data = np.array(data, dtype=\"float\")\n",
    "    #print(data.shape)\n",
    "\n",
    "    # scale the raw pixel intensities to the range [0, 1]\n",
    "    train_y = np.array(train_y)\n",
    "    print(train_y.shape)\n",
    "    # 整数编码\n",
    "    integer_result = label_encoder.transform(train_y)\n",
    "    # One-Hot编码\n",
    "    #one_hot_encoder = OneHotEncoder()\n",
    "# One-Hot编码将分类值映射到整数值再表示成二进制向量\n",
    "    integer_result1 = integer_result.reshape(len(integer_result), 1)\n",
    "    y_label = one_hot_encoder.transform(integer_result1)\n",
    "\n",
    "    #labels = np.array(labels)\n",
    "\n",
    "    # convert the labels from integers to vectors\n",
    "    #labels = to_categorical(labels, num_classes=CLASS_NUM)                         \n",
    "    return data,y_label\n",
    "    \n",
    "part='val'\n",
    "valX,valY = load_data(path='backup/',p=part) \n",
    "val_Y=valY.toarray()\n",
    "valX = np.reshape(valX,(-1,120,120,29,3))\n",
    "# part='test'\n",
    "# testX,testY = load_data(path='backup/',p=part) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 29, 120, 120, 3)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 29, 60, 60, 64)    47104     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 29, 60, 60, 64)    256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 29, 30, 30, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 29, 28, 28, 128)   73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 29, 28, 28, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 29, 14, 14, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 29, 12, 12, 256)   295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 29, 12, 12, 256)   1024      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 29, 6, 6, 256)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 29, 4, 4, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 29, 4, 4, 512)     2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 29, 2, 2, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 29, 2048)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 29, 256)           2229248   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 29, 256)           394240    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 4,497,030\n",
      "Trainable params: 4,493,062\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Model(inputs=[input1], \n",
    "              outputs=[dense2])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "# model = SmallerVGGNet.build(width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
    "# \tdepth=IMAGE_DIMS[2], classes=6)\n",
    "#opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9)\n",
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 5 dimensions, but got array with shape (900, 120, 120, 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c1d418a30451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_number\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                    callbacks = callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2158\u001b[0m                                      str(validation_data))\n\u001b[1;32m   2159\u001b[0m                 val_x, val_y, val_sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 2160\u001b[0;31m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m   2161\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 5 dimensions, but got array with shape (900, 120, 120, 29)"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training network...\")\n",
    "sample_number = len(train_list)\n",
    "val_number = len(val_list)\n",
    "#print(sample_number)\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,\n",
    "mode='max')\n",
    "stopper = EarlyStopping(monitor='acc',patience=3)\n",
    "callbacks_list = [checkpoint,stopper]\n",
    "\n",
    "model1.fit_generator(generate_arrays_from_file(train_list,batch_size=32),\n",
    "                        validation_data=(valX,val_Y),\n",
    "                        steps_per_epoch=sample_number // 32,\n",
    "                        epochs=10, verbose=1,\n",
    "                   callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
